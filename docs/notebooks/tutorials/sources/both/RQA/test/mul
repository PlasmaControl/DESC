#!/bin/bash
#SBATCH --job-name=mul_pinv
#SBATCH --time=00:30:00
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=28G
#SBATCH --mail-type=begin        # send mail when process begins
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-user=fcastro@princeton.edu

# Load your modules or activate your environment
module purge
module load anaconda3/2023.9
source activate desc-ray  # Replace with your actual env name

### Threading environment
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OPENBLAS_NUM_THREADS=$SLURM_CPUS_PER_TASK
export NUMEXPR_NUM_THREADS=$SLURM_CPUS_PER_TASK

### Ray setup
head_node=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
port=6379

echo "Head node: $head_node"
echo "Starting Ray head..."

if [ "$SLURM_NODEID" -eq 0 ]; then
    # Launch Ray head on the first node
    ray start --head --node-ip-address=$head_node --port=$port \
        --num-cpus=$SLURM_CPUS_PER_TASK \
        --block &
else
    # Launch Ray worker on the other nodes
    ray start --address=$head_node:$port \
        --num-cpus=$SLURM_CPUS_PER_TASK \
        --block &
fi

### Wait a little to ensure cluster is up
sleep 30

### Run the distributed Python script only on head node
if [ "$SLURM_NODEID" -eq 0 ]; then
    jupyter nbconvert --ExecutePreprocessor.timeout=None --to notebook --execute --inplace mul.ipynb
#python bn_res_vec_ray.py
fi

wait
